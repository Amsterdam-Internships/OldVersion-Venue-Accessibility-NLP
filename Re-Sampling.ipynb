{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9955f57c",
   "metadata": {},
   "source": [
    "# Re-sampling Imbalanced Training Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42af79c9",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/re-sampling-imbalanced-training-corpus-for-sentiment-analysis-c9dc97f9eae1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a5be3d",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd0d74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn import model_selection, preprocessing, metrics, linear_model, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from imblearn.over_sampling import BorderlineSMOTE, SMOTE, ADASYN, SMOTENC, RandomOverSampler\n",
    "from imblearn.under_sampling import (RandomUnderSampler, \n",
    "                                    NearMiss, \n",
    "                                    InstanceHardnessThreshold,\n",
    "                                    CondensedNearestNeighbour,\n",
    "                                    EditedNearestNeighbours,\n",
    "                                    RepeatedEditedNearestNeighbours,\n",
    "                                    AllKNN,\n",
    "                                    NeighbourhoodCleaningRule,\n",
    "                                    OneSidedSelection,\n",
    "                                    TomekLinks)\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import make_pipeline\n",
    "import pandas as pd, numpy, string\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "#Remove Special Charactors\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599909b1",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f47788f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rn/k7vf4gp97fb11dzjbcr7w8bc0000gn/T/ipykernel_73893/1735966122.py:1: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv(\"./Data/train_data.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Venue Index</th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Rating Sent</th>\n",
       "      <th>Text</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Accessibility</th>\n",
       "      <th>Term1</th>\n",
       "      <th>Term2</th>\n",
       "      <th>Term3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18594</th>\n",
       "      <td>10</td>\n",
       "      <td>coffeeshop la grotte</td>\n",
       "      <td>2019</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>positive</td>\n",
       "      <td>2nd year coming here and can t say enough good...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373579</th>\n",
       "      <td>451</td>\n",
       "      <td>pata negra</td>\n",
       "      <td>2019</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>positive</td>\n",
       "      <td>excellent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128693</th>\n",
       "      <td>112</td>\n",
       "      <td>bagels &amp; beans ijdock</td>\n",
       "      <td>2021</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>positive</td>\n",
       "      <td>location, very nice and welcoming, with a spl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120681</th>\n",
       "      <td>17</td>\n",
       "      <td>plum</td>\n",
       "      <td>2020</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>positive</td>\n",
       "      <td>went to this place on saturday. i had the gril...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162003</th>\n",
       "      <td>504</td>\n",
       "      <td>caf‚àö¬© thijssen</td>\n",
       "      <td>2021</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>positive</td>\n",
       "      <td>was wer very tasty</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Venue Index                   Name  Date     Rating Rating Sent  \\\n",
       "Index                                                                     \n",
       "18594            10   coffeeshop la grotte  2019   5 stars     positive   \n",
       "373579          451             pata negra  2019   5 stars     positive   \n",
       "128693          112  bagels & beans ijdock  2021   5 stars     positive   \n",
       "120681           17                   plum  2020   5 stars     positive   \n",
       "162003          504      caf‚àö¬© thijssen  2021   5 stars     positive   \n",
       "\n",
       "                                                     Text  Aspect  \\\n",
       "Index                                                               \n",
       "18594   2nd year coming here and can t say enough good...       0   \n",
       "373579                                          excellent       0   \n",
       "128693   location, very nice and welcoming, with a spl...       0   \n",
       "120681  went to this place on saturday. i had the gril...       0   \n",
       "162003                                 was wer very tasty       0   \n",
       "\n",
       "        Accessibility Term1 Term2 Term3  \n",
       "Index                                    \n",
       "18594               0   NaN   NaN   NaN  \n",
       "373579              0   NaN   NaN   NaN  \n",
       "128693              0   NaN   NaN   NaN  \n",
       "120681              0   NaN   NaN   NaN  \n",
       "162003              0   NaN   NaN   NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./Data/train_data.csv\")\n",
    "train = train.set_index(\"Index\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63862a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLITTING DATA\n",
    "\n",
    "# Split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train['Text'],train['Accessibility'])\n",
    "\n",
    "# Label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6bf6093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  99.52676918166836 %\n",
      "0:  0.47323081833164127 %\n"
     ]
    }
   ],
   "source": [
    "# DATA DISTRIBUTION\n",
    "\n",
    "# Percentage of Positive/Negative\n",
    "print(\"1: \", train.Accessibility.value_counts()[0]/len(train)*100,\"%\")\n",
    "print(\"0: \", train.Accessibility.value_counts()[1]/len(train)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30bda55",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd3f5adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100000)\n",
    "tfidf_vect.fit(train['Text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd314d9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d0973c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the f1 Score\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    return metrics.f1_score(valid_y,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4be4f89",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "970f2fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Baseline, WordLevel TFIDF:  0.7912087912087912\n",
      "SVM Baseline, WordLevel TFIDF:  0.9031446540880502\n"
     ]
    }
   ],
   "source": [
    "# ACCURARY\n",
    "\n",
    "accuracyORIGINAL = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"LR Baseline, WordLevel TFIDF: \", accuracyORIGINAL)\n",
    "accuracyORIGINAL = train_model(svm.LinearSVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"SVM Baseline, WordLevel TFIDF: \", accuracyORIGINAL)\n",
    "#LR Baseline, WordLevel TFIDF:  0.7912087912087912\n",
    "#SVM Baseline, WordLevel TFIDF:  0.9031446540880502"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb33ae",
   "metadata": {},
   "source": [
    "## Re-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cf1558e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR ORIGINAL, WordLevel TFIDF:  0.851063829787234\n",
      "SVM ROS, WordLevel TFIDF:  0.880093131548312\n"
     ]
    }
   ],
   "source": [
    "#Random Over Sampling\n",
    "ros = RandomOverSampler(random_state=777)\n",
    "ros_xtrain_tfidf, ros_train_y = ros.fit_resample(xtrain_tfidf, train_y)\n",
    "accuracyROS = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf)\n",
    "print (\"LR ORIGINAL, WordLevel TFIDF: \", accuracyROS)\n",
    "accuracyROS = train_model(svm.LinearSVC(),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf)\n",
    "print (\"SVM ROS, WordLevel TFIDF: \", accuracyROS)\n",
    "#LR ROS, WordLevel TFIDF:  0.851063829787234\n",
    "#SVM ROS, WordLevel TFIDF:  0.880093131548312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b239ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR SMOTE, WordLevel TFIDF:  0.8515709642470205\n",
      "SVC SMOTE, WordLevel TFIDF:  0.8787878787878787\n"
     ]
    }
   ],
   "source": [
    "#SMOTE\n",
    "sm = SMOTE(random_state=777)#, ratio = 1.0)\n",
    "sm_xtrain_tfidf, sm_train_y = sm.fit_resample(xtrain_tfidf, train_y)\n",
    "accuracySMOTE = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),sm_xtrain_tfidf, sm_train_y, xvalid_tfidf)\n",
    "print (\"LR SMOTE, WordLevel TFIDF: \", accuracySMOTE)\n",
    "accuracySMOTE = train_model(svm.LinearSVC(),sm_xtrain_tfidf, sm_train_y, xvalid_tfidf)\n",
    "print (\"SVC SMOTE, WordLevel TFIDF: \", accuracySMOTE)\n",
    "#LR SMOTE, WordLevel TFIDF:  0.6848436246992782\n",
    "#SVC SMOTE, WordLevel TFIDF:  0.693288020390824"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
